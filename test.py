# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136UBnhSxqEdF1A_q6kYA2K0SW9d8yuXw
"""

# Diabetes/Medical Dataset ML Pipeline
# -----------------------------
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import GridSearchCV

df = pd.read_csv('./datasets/MulticlassDiabetesDataset.csv')

# 2. Handle Categorical Columns
# -----------------------------
if df['Gender'].dtype == 'object':
    df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})

# 3. Handle Missing Values
# -----------------------------
imputer = KNNImputer(n_neighbors=5)
df[df.columns] = imputer.fit_transform(df)

# 4. Feature Engineering
# -----------------------------
df['TG_HDL_Ratio'] = df['TG'] / (df['HDL'] + 1)  # avoid division by zero
df['BMI_Age'] = df['BMI'] * df['AGE']
df['LDL_HDL_Ratio'] = df['LDL'] / (df['HDL'] + 1)

# 5. Split Features & Target
# -----------------------------
X = df.drop('Class', axis=1)
y = df['Class']

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# -----------------------------
# 1. Define the classifier
# -----------------------------
lr = LogisticRegression(max_iter=1000, random_state=42)

# -----------------------------
# 2. Define hyperparameter grid
# -----------------------------
lr_params = {
    'C': [0.01, 0.1, 1, 10, 100],  # inverse of regularization strength
    'penalty': ['l2'],             # l2 regularization
    'solver': ['lbfgs']            # solver compatible with l2
}

# -----------------------------
# 3. Grid Search
# -----------------------------
lr_grid = GridSearchCV(
    estimator=lr,
    param_grid=lr_params,
    cv=5,                # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1
)

# -----------------------------
# 4. Fit to training data
# -----------------------------
lr_grid.fit(X_train, y_train)

# -----------------------------
# 5. Get best Logistic Regression model
# -----------------------------
lr_best = lr_grid.best_estimator_

# -----------------------------
# 6. Evaluate on test set
# -----------------------------
y_pred = lr_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)


print("Test Accuracy (Logistic Regression):", accuracy)

rf = RandomForestClassifier(random_state=42)

rf_params = {'n_estimators':[100,200], 'max_depth':[3,5,7], 'min_samples_split':[2,5]}
rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='accuracy')
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_

rf_best.fit(X_train, y_train)

y_pred = rf_best.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Test Accuracy (Random Forest):", accuracy)

xgb = XGBClassifier(
    n_estimators=200, learning_rate=0.05, max_depth=3, eval_metric='logloss', random_state=42
)

xgb_params = {'n_estimators':[100,200], 'max_depth':[3,4], 'learning_rate':[0.05,0.1]}
xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='accuracy')
xgb_grid.fit(X_train, y_train)
xgb_best = xgb_grid.best_estimator_

xgb_best.fit(X_train, y_train)

y_pred = xgb_best.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Test Accuracy (Random Forest):", accuracy)

# 6. Define Base Models
# -----------------------------
rf = RandomForestClassifier(random_state=42)
xgb = XGBClassifier(
    n_estimators=200, learning_rate=0.05, max_depth=3, eval_metric='logloss', random_state=42
)
lr = LogisticRegression(max_iter=1000, random_state=42)

# 7. Hyperparameter Tuning (Optional, basic)
# -----------------------------
rf_params = {'n_estimators':[100,200], 'max_depth':[3,5,7], 'min_samples_split':[2,5]}
rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='accuracy')
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_

xgb_params = {'n_estimators':[100,200], 'max_depth':[3,4], 'learning_rate':[0.05,0.1]}
xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='accuracy')
xgb_grid.fit(X_train, y_train)
xgb_best = xgb_grid.best_estimator_

# 8. Voting Ensemble
# -----------------------------
voting_clf = VotingClassifier(
    estimators=[('rf', rf_best), ('xgb', xgb_best), ('lr', lr)],
    voting='soft',  # use probabilities
    weights=[2,3,1]  # XGBoost stronger, give more weight
)
voting_clf.fit(X_train, y_train)

# 9. Evaluate train Accuracy
# -----------------------------
y_pred = voting_clf.predict(X_train)
t_accuracy = accuracy_score(y_train, y_pred)
print("train Accuracy:", t_accuracy)

# 9. Evaluate Test Accuracy
# -----------------------------
y_pred = voting_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Test Accuracy:", accuracy)